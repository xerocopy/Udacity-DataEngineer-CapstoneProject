Data Engineering Capstone Project
An automated data pipeling for US immigration information.


Purpose of this project:
The goal of the project is to build a ETL pipeline to run the immigration and climate data of US. The data pipeline extracts temperature, airport, immigration and demographic data from various websites with Apache Airflow. Firstly, the json files is migrate to s3. Then the data is uploaded to Redshift. Following that, the data is transformed and loaded to normalized fact and dimentional tables before they get checked for the discrepancies found in the data. 


Datasets used in the project:
1. I94 Immigration Data: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. https://www.trade.gov/national-travel-and-tourism-office
2. World Temperature Data: This dataset came from Kaggle. https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data
3. U.S. City Demographic Data: This data comes from OpenSoft. https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/
4. Airport Code Table: This is a simple table of airport codes and corresponding cities. https://datahub.io/core/airport-codes#data

These data are now available in my S3 buckets as well: arn:aws:s3:::myudacitycapstionprojectbucket


Scope of the Project and End User Cases: 
Depending on the following use cases the data is collected:
1. The data can be used to identify the impact that changes in temperature have on city population, going even further to categorize the impact by gender.
2. Immigration researchers can use the data to identify what some of the most common ports of entry are, type of visas that are commonly used, use the arrival and departure times to identify periods of high traffic at the ports of entry, the impact of temperature on the rate on immigration by city.
(e.g., analytics table, app back-end, source-of-truth database, etc.)



Data modeling
***City Fact Table***
Table Column			Data Type		Description
city_id (PRIMARY_KEY)		varchar(32)		auto generated primary key
city_name			varchar(32)		name of city
country				varchar(32)		name of country
latitude			varchar(10)		latitude value
longitude			varchar(10)		longitude value
average_temperature		numeric			average temperature of the city
date				date			date of temperature recording


***Airport Dimension Table***
Table Column			Data Type		Description
airport_id (PRIMARY_KEY)	varchar(50)		auto generated primary key
airport_code			varchar(50)		airport short code
name				varchar(500)		name of airport
continent			varchar(50)		continent code
country_code			varchar(32)		country code
state				varchar(32)		state code

***Visitor Dimension Table***
Table Column			Data Type		Description
visitor_id (PRIMARY_KEY)	varchar(32)		auto generated primary key
year				int4			year of visit
month				int4			month of visit
city				varchar(32)		city of visit
gender				varchar(32)		gender of visitor
arrival_date			date			arrival date of the visitor
departure_date			date			departure time of the visitor
airline				varchar(32)		airline code

***Demographic Dimension Table***
Table Column			Data Type		Description
demographic_id (PRIMARY_KEY)	varchar(100)		auto generated primary key
city				varchar(50)		city name
state				varchar(50)		state code
male_population	int4		male 			population numbers by city
female_population		int4			female population numbers by city
total_population		int4			total population numbers by city




## Tools and Technologies used
The tools used in this project include:
- __Apache Spark__ - This was needed to process data from the big data SAS and csv files to dataframes and convert them to the more readable json data files. In the process, it maps the columns from the datasets to the relevant columns required for the staging tables and also maps some of the values such as the city codes from the provided `label descriptions` data dictionary to city names in the data generated by spark. To view how this is done, check out this [python helper file](additional_helpers/data_to_json.py) and the data dictionary for [city codes](data/label_descriptions/city_codes.json).
- __Apache Airflow__ - This was required to automate workflows, namely uploading processed json files from the local filesystem to s3, creating the staging, fact and dimension tables, copying the s3 files to the redshift staging table then performing ETL to load the final data to the fact and dimension tables. This is all done by pre-configured dags, the [file upload dag](dags/file_upload_dag.py) and the [capstone dag](dags/capstone_dag.py) that both perform a series of tasks.
- __Amazon Redshift__ - The database is located in a redshift cluster that store the data from s3 and the eventual data that gets added to the fact and dimension tables.
- __Amazon S3__ - Stores the json files generated by spark that are uploaded from the local filesystem.





